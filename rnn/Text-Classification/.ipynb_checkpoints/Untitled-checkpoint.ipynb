{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pickle'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops.math_ops import sigmoid\n",
    "from tensorflow.python.ops.math_ops import tanh\n",
    "from tensorflow.contrib.rnn.python.ops import core_rnn_cell\n",
    "\n",
    "import pickle\n",
    "pickle.__name__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1, 'b': 2}\n"
     ]
    }
   ],
   "source": [
    "# cPickle is a faster implementation of the pickle module in C\n",
    "import pickle\n",
    "d = {\"a\": 1, \"b\": 2}\n",
    "with open(\"someobject.pickle\", \"wb\") as output_file:\n",
    "    pickle.dump(d, output_file)\n",
    "    \n",
    "with open(\"someobject.pickle\", \"rb\") as input_file:\n",
    "    e = pickle.load(input_file)\n",
    "\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_file: utils/labels.pkl\n",
      "label_file type: <_io.BufferedReader name='utils/labels.pkl'>\n",
      "<class '_io.BufferedReader'>\n",
      "labels: {'A': 0, 'C': 1, 'F': 2, 'N': 3, 'P': 4, 'U': 5, 'X': 6}\n"
     ]
    }
   ],
   "source": [
    "utils_dir = \"utils\"\n",
    "\n",
    "label_file = os.path.join(utils_dir, 'labels.pkl')\n",
    "vocab_file = os.path.join(utils_dir, 'vocab.pkl')\n",
    "corpus_file = os.path.join(utils_dir, 'corpus.txt')\n",
    "print('label_file: {}'.format(label_file))\n",
    "\n",
    "with open(label_file, 'rb') as f:\n",
    "    print('label_file type: {}'.format(f))\n",
    "    print(type(f))\n",
    "    labels = pickle.load(f)\n",
    "label_size = len(labels)\n",
    "print('labels: {}'.format(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "a bytes-like object is required, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/data/loaddata/ljhuang/mi/TensorFlowLab/rnn/Text-Classification/train.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data/loaddata/ljhuang/mi/TensorFlowLab/rnn/Text-Classification/train.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/loaddata/ljhuang/mi/TensorFlowLab/rnn/Text-Classification/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontinue_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0mdata_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/loaddata/ljhuang/mi/TensorFlowLab/rnn/Text-Classification/utils.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, is_training, utils_dir, data_path, batch_size, seq_length, vocab, labels, encoding)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'label_file: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'label_file type: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: a bytes-like object is required, not 'str'"
     ]
    }
   ],
   "source": [
    "% run train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "a bytes-like object is required, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/data/loaddata/ljhuang/mi/TensorFlowLab/rnn/Text-Classification/train.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data/loaddata/ljhuang/mi/TensorFlowLab/rnn/Text-Classification/train.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/loaddata/ljhuang/mi/TensorFlowLab/rnn/Text-Classification/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontinue_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0mdata_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/loaddata/ljhuang/mi/TensorFlowLab/rnn/Text-Classification/utils.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, is_training, utils_dir, data_path, batch_size, seq_length, vocab, labels, encoding)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'label_file: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'label_file type: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: a bytes-like object is required, not 'str'"
     ]
    }
   ],
   "source": [
    "% run train.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os, sys\n",
    "import time\n",
    "import csv\n",
    "import collections\n",
    "# import cPickle as pickle\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import learn\n",
    "\n",
    "\n",
    "class TextLoader(object):\n",
    "    def __init__(self, is_training, utils_dir, data_path, batch_size, seq_length, vocab, labels, encoding='utf8'):\n",
    "        self.data_path = data_path\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_length = seq_length\n",
    "        self.encoding = encoding\n",
    "\n",
    "        if is_training:\n",
    "            self.utils_dir = utils_dir\n",
    "\n",
    "            label_file = os.path.join(utils_dir, 'labels.pkl')\n",
    "            vocab_file = os.path.join(utils_dir, 'vocab.pkl')\n",
    "            corpus_file = os.path.join(utils_dir, 'corpus.txt')\n",
    "            print('label_file: {}'.format(label_file))\n",
    "            \n",
    "            with open(label_file, 'r') as f:\n",
    "                print('label_file type: {}'.format(f))\n",
    "                self.labels = pickle.load(f)\n",
    "            self.label_size = len(self.labels)\n",
    "\n",
    "            if not os.path.exists(vocab_file):\n",
    "                print('reading corpus and processing data')\n",
    "                self.preprocess(vocab_file, corpus_file, data_path)\n",
    "            else:\n",
    "                print('loading vocab and processing data')\n",
    "                self.load_preprocessed(vocab_file, data_path)\n",
    "\n",
    "        elif vocab is not None and labels is not None:\n",
    "            self.vocab = vocab\n",
    "            self.vocab_size = len(vocab) + 1\n",
    "            self.labels = labels\n",
    "            self.label_size = len(self.labels)\n",
    "\n",
    "            self.load_preprocessed(None, data_path)\n",
    "\n",
    "        self.reset_batch_pointer()\n",
    "\n",
    "\n",
    "    def transform(self, d):\n",
    "        new_d = map(self.vocab.get, d[:self.seq_length])\n",
    "        new_d = map(lambda i: i if i else 0, new_d)\n",
    "\n",
    "        if len(new_d) >= self.seq_length:\n",
    "            new_d = new_d[:self.seq_length]\n",
    "        else:\n",
    "            new_d = new_d + [0] * (self.seq_length - len(new_d))\n",
    "        return new_d\n",
    "\n",
    "\n",
    "    def preprocess(self, vocab_file, corpus_file, data_path):\n",
    "        with open(corpus_file, 'rb') as f:\n",
    "            corpus = f.readlines()\n",
    "            corpus = ''.join(map(lambda i: i.strip(), corpus))\n",
    "\n",
    "        try:\n",
    "            corpus = corpus.decode('utf8')\n",
    "        except Exception as e:\n",
    "            # print e\n",
    "            pass\n",
    "\n",
    "        counter = collections.Counter(corpus)\n",
    "        count_pairs = sorted(counter.items(), key=lambda i: -i[1])\n",
    "        self.chars, _ = zip(*count_pairs)\n",
    "        with open(vocab_file, 'wb') as f:\n",
    "            pickle.dump(self.chars, f)\n",
    "\n",
    "        self.vocab_size = len(self.chars) + 1\n",
    "        self.vocab = dict(zip(self.chars, range(1, len(self.chars)+1)))\n",
    "\n",
    "        data = pd.read_csv(data_path, encoding='utf8')\n",
    "        tensor_x = np.array(list(map(self.transform, data['text'])))\n",
    "        tensor_y = np.array(list(map(self.labels.get, data['label'])))\n",
    "        self.tensor = np.c_[tensor_x, tensor_y].astype(int)\n",
    "\n",
    "\n",
    "    def load_preprocessed(self, vocab_file, data_path):\n",
    "        if vocab_file is not None:\n",
    "            with open(vocab_file, 'rb') as f:\n",
    "                self.chars = pickle.load(f)\n",
    "            self.vocab_size = len(self.chars) + 1\n",
    "            self.vocab = dict(zip(self.chars, range(1, len(self.chars)+1)))\n",
    "\n",
    "        data = pd.read_csv(data_path, encoding='utf8')\n",
    "        tensor_x = np.array(list(map(self.transform, data['text'])))\n",
    "        tensor_y = np.array(list(map(self.labels.get, data['label'])))\n",
    "        self.tensor = np.c_[tensor_x, tensor_y].astype(int)\n",
    "\n",
    "\n",
    "    def create_batches(self):\n",
    "        self.num_batches = int(self.tensor.shape[0] / self.batch_size)\n",
    "        if self.num_batches == 0:\n",
    "            assert False, 'Not enough data, make batch_size small.'\n",
    "\n",
    "        np.random.shuffle(self.tensor)\n",
    "        tensor = self.tensor[:self.num_batches * self.batch_size]\n",
    "        self.x_batches = np.split(tensor[:, :-1], self.num_batches, 0)\n",
    "        self.y_batches = np.split(tensor[:, -1], self.num_batches, 0)\n",
    "\n",
    "\n",
    "    def next_batch(self):\n",
    "        x = self.x_batches[self.pointer]\n",
    "        y = self.y_batches[self.pointer]\n",
    "        self.pointer += 1\n",
    "        return x, y\n",
    "\n",
    "\n",
    "    def reset_batch_pointer(self):\n",
    "        self.create_batches()\n",
    "        self.pointer = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--utils_dir UTILS_DIR]\n",
      "                             [--data_path DATA_PATH] [--save_dir SAVE_DIR]\n",
      "                             [--model MODEL] [--rnn_size RNN_SIZE]\n",
      "                             [--num_layers NUM_LAYERS]\n",
      "                             [--batch_size BATCH_SIZE]\n",
      "                             [--seq_length SEQ_LENGTH]\n",
      "                             [--num_epochs NUM_EPOCHS]\n",
      "                             [--save_every SAVE_EVERY]\n",
      "                             [--learning_rate LEARNING_RATE]\n",
      "                             [--decay_rate DECAY_RATE]\n",
      "                             [--continue_training CONTINUE_TRAINING]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /run/user/0/jupyter/kernel-3b415bc5-3477-41cf-8f3e-16f8b43047bc.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2889: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os, sys\n",
    "import time\n",
    "import argparse\n",
    "# import cPickle as pickle\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from utils import TextLoader\n",
    "from model import Model\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--utils_dir', type=str, default='utils',\n",
    "                        help='''directory containing labels.pkl and corpus.txt:\n",
    "                        'corpus.txt'      : corpus to define vocabulary;\n",
    "                        'vocab.pkl'       : vocabulary definitions;\n",
    "                        'labels.pkl'      : label definitions''')\n",
    "\n",
    "    parser.add_argument('--data_path', type=str, default='data/train.csv',\n",
    "                        help='data to train model')\n",
    "\n",
    "    parser.add_argument('--save_dir', type=str, default='save',\n",
    "                        help='directory to store checkpointed models')\n",
    "\n",
    "    parser.add_argument('--model', type=str, default='lstm',\n",
    "                        help='rnn, gru, lstm or bn-lstm, default lstm')\n",
    "\n",
    "    parser.add_argument('--rnn_size', type=int, default=128,\n",
    "                        help='size of RNN hidden state')\n",
    "\n",
    "    parser.add_argument('--num_layers', type=int, default=2,\n",
    "                        help='number of layers in RNN')\n",
    "\n",
    "    parser.add_argument('--batch_size', type=int, default=128,\n",
    "                        help='minibatch size')\n",
    "\n",
    "    parser.add_argument('--seq_length', type=int, default=20,\n",
    "                        help='RNN sequence length')\n",
    "\n",
    "    parser.add_argument('--num_epochs', type=int, default=100,\n",
    "                        help='number of epochs')\n",
    "\n",
    "    parser.add_argument('--save_every', type=int, default=100,\n",
    "                        help='save frequency')\n",
    "\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.001,\n",
    "                        help='learning rate')\n",
    "\n",
    "    parser.add_argument('--decay_rate', type=float, default=0.9,\n",
    "                        help='decay rate for rmsprop')\n",
    "\n",
    "    parser.add_argument('--continue_training', type=str, default='False',\n",
    "                        help='whether to continue training.')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    train(args)\n",
    "\n",
    "\n",
    "def train(args):\n",
    "    if args.continue_training in ['True', 'true']:\n",
    "        args.continue_training = True\n",
    "    else:\n",
    "        args.continue_training = False\n",
    "\n",
    "    data_loader = TextLoader(True, args.utils_dir, args.data_path, args.batch_size, args.seq_length, None, None)\n",
    "    args.vocab_size = data_loader.vocab_size\n",
    "    args.label_size = data_loader.label_size\n",
    "\n",
    "    if args.continue_training:\n",
    "        assert os.path.isfile(os.path.join(args.save_dir, 'config.pkl')), 'config.pkl file does not exist in path %s' % args.save_dir\n",
    "        assert os.path.isfile(os.path.join(args.utils_dir, 'chars_vocab.pkl')), 'chars_vocab.pkl file does not exist in path %s' % args.utils_dir\n",
    "        assert os.path.isfile(os.path.join(args.utils_dir, 'labels.pkl')), 'labels.pkl file does not exist in path %s' % args.utils_dir\n",
    "        ckpt = tf.train.get_checkpoint_state(args.save_dir)\n",
    "        assert ckpt, 'No checkpoint found'\n",
    "        assert ckpt.model_checkpoint_path, 'No model path found in checkpoint'\n",
    "\n",
    "        with open(os.path.join(args.save_dir, 'config.pkl'), 'rb') as f:\n",
    "            saved_model_args = pickle.load(f)\n",
    "        need_be_same = ['model', 'rnn_size', 'num_layers', 'seq_length']\n",
    "        for checkme in need_be_same:\n",
    "            assert vars(saved_model_args)[checkme]==vars(args)[checkme], 'command line argument and saved model disagree on %s' % checkme\n",
    "\n",
    "        with open(os.path.join(args.utils_dir, 'chars_vocab.pkl'), 'rb') as f:\n",
    "            saved_chars, saved_vocab = pickle.load(f)\n",
    "        with open(os.path.join(args.utils_dir, 'labels.pkl'), 'rb') as f:\n",
    "            saved_labels = pickle.load(f)\n",
    "        assert saved_chars==data_loader.chars, 'data and loaded model disagree on character set'\n",
    "        assert saved_vocab==data_loader.vocab, 'data and loaded model disagree on dictionary mappings'\n",
    "        assert saved_labels==data_loader.labels, 'data and loaded model disagree on label dictionary mappings'\n",
    "\n",
    "    with open(os.path.join(args.save_dir, 'config.pkl'), 'wb') as f:\n",
    "        pickle.dump(args, f)\n",
    "    with open(os.path.join(args.utils_dir, 'chars_vocab.pkl'), 'wb') as f:\n",
    "        pickle.dump((data_loader.chars, data_loader.vocab), f)\n",
    "    with open(os.path.join(args.utils_dir, 'labels.pkl'), 'wb') as f:\n",
    "        pickle.dump(data_loader.labels, f)\n",
    "\n",
    "    model = Model(args)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        init = tf.initialize_all_variables()\n",
    "        sess.run(init)\n",
    "        saver = tf.train.Saver(tf.all_variables())\n",
    "\n",
    "        if args.continue_training:\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "        for e in range(args.num_epochs):\n",
    "            sess.run(tf.assign(model.lr, args.learning_rate * (args.decay_rate ** e)))\n",
    "            data_loader.reset_batch_pointer()\n",
    "\n",
    "            for b in range(data_loader.num_batches):\n",
    "                start = time.time()\n",
    "                x, y = data_loader.next_batch()\n",
    "                feed = {model.input_data: x, model.targets: y}\n",
    "                train_loss, state, _, accuracy = sess.run([model.cost, model.final_state, model.optimizer, model.accuracy], feed_dict=feed)\n",
    "                end = time.time()\n",
    "                print('{}/{} (epoch {}), train_loss = {:.3f}, accuracy = {:.3f}, time/batch = {:.3f}'\\\n",
    "                    .format(e * data_loader.num_batches + b + 1,\n",
    "                            args.num_epochs * data_loader.num_batches,\n",
    "                            e + 1,\n",
    "                            train_loss,\n",
    "                            accuracy,\n",
    "                            end - start))\n",
    "                if (e*data_loader.num_batches+b+1) % args.save_every == 0 \\\n",
    "                    or (e==args.num_epochs-1 and b==data_loader.num_batches-1):\n",
    "                    checkpoint_path = os.path.join(args.save_dir, 'model.ckpt')\n",
    "                    saver.save(sess, checkpoint_path, global_step=e*data_loader.num_batches+b+1)\n",
    "                    print('model saved to {}'.format(checkpoint_path))\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
