{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 19.3765831    0.           0.           0.           0.           0.\n",
      "    0.           0.        ]\n",
      " [  0.29494491  25.95082092   0.           0.           0.           0.\n",
      "    0.           0.        ]\n",
      " [ 25.2746582    5.78165054   9.9505682    0.           0.           0.\n",
      "    0.           0.        ]\n",
      " [ 24.15919495   8.90028667   9.38859081  24.47133827   1.11007941\n",
      "    3.17366886   0.49285793  24.88559151]]\n",
      "[ 19.3765831   13.12288284  13.66895962  12.0727005 ]\n",
      "14.5603\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "tf.set_random_seed(10)\n",
    "np.random.seed(10)\n",
    "\n",
    "# Batch size\n",
    "B = 4\n",
    "# (Maximum) number of time steps in this batch\n",
    "T = 8\n",
    "RNN_DIM = 128\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "# The *acutal* length of the examples\n",
    "example_len = [1, 2, 3, 8]\n",
    "\n",
    "# The classes of the examples at each step (between 1 and 9, 0 means padding)\n",
    "y = np.random.randint(1, 10, [B, T])\n",
    "for i, length in enumerate(example_len):\n",
    "    y[i, length:] = 0   \n",
    "    \n",
    "# The RNN outputs\n",
    "rnn_outputs = tf.convert_to_tensor(np.random.randn(B, T, RNN_DIM), dtype=tf.float32)\n",
    "\n",
    "# Output layer weights\n",
    "W = tf.get_variable(\n",
    "    name=\"W\",\n",
    "    initializer=tf.random_normal_initializer(),\n",
    "    shape=[RNN_DIM, NUM_CLASSES])\n",
    "\n",
    "# Calculate logits and probs\n",
    "# Reshape so we can calculate them all at once\n",
    "rnn_outputs_flat = tf.reshape(rnn_outputs, [-1, RNN_DIM])\n",
    "logits_flat = tf.batch_matmul(rnn_outputs_flat, W)\n",
    "probs_flat = tf.nn.softmax(logits_flat)\n",
    "\n",
    "# Calculate the losses \n",
    "y_flat =  tf.reshape(y, [-1])\n",
    "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits_flat, y_flat)\n",
    "\n",
    "# Mask the losses\n",
    "mask = tf.sign(tf.to_float(y_flat))\n",
    "masked_losses = mask * losses\n",
    "\n",
    "# Bring back to [B, T] shape\n",
    "masked_losses = tf.reshape(masked_losses,  tf.shape(y))\n",
    "\n",
    "# Calculate mean loss\n",
    "mean_loss_by_example = tf.reduce_sum(masked_losses, reduction_indices=1) / example_len\n",
    "mean_loss = tf.reduce_mean(mean_loss_by_example)\n",
    "\n",
    "result = tf.contrib.learn.run_n(\n",
    "    {\n",
    "        \"masked_losses\": masked_losses,\n",
    "        \"mean_loss_by_example\": mean_loss_by_example,\n",
    "        \"mean_loss\": mean_loss\n",
    "    },\n",
    "    n=1,\n",
    "    feed_dict=None)\n",
    "\n",
    "print(result[0][\"masked_losses\"])\n",
    "print(result[0][\"mean_loss_by_example\"])\n",
    "print(result[0][\"mean_loss\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
